{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains code that was used to extract chosen variables to be used as features later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the dataset\n",
    "dataset = pd.read_csv(\"/Users/maks/Documents/ML_project/Met Dataset 2015-2022.csv\")\n",
    "\n",
    "#Removes rows with any NaN values\n",
    "cleaned_dataset = dataset.dropna()\n",
    "\n",
    "#List specifying which columns to keep\n",
    "columns = [\"x_coord\", \"y_coord\", \"year\", \n",
    "           \"hurs_1\", \"hurs_2\", \"hurs_3\", \"hurs_4\", \"hurs_5\", \"hurs_6\", \"hurs_7\", \"hurs_8\", \"hurs_9\", \"hurs_10\", \"hurs_11\", \"hurs_12\",\n",
    "           \"psl_1\", \"psl_2\", \"psl_3\", \"psl_4\", \"psl_5\", \"psl_6\", \"psl_7\", \"psl_8\", \"psl_9\", \"psl_10\", \"psl_11\",\"psl_12\",\n",
    "           \"tas_1\", \"tas_2\", \"tas_3\", \"tas_4\", \"tas_5\", \"tas_6\", \"tas_7\", \"tas_8\", \"tas_9\", \"tas_10\", \"tas_11\", \"tas_12\", \n",
    "           \"rainfall_1\", \"rainfall_2\", \"rainfall_3\", \"rainfall_4\", \"rainfall_5\", \"rainfall_6\", \"rainfall_7\", \"rainfall_8\", \"rainfall_9\", \"rainfall_10\", \"rainfall_11\", \"rainfall_12\"]\n",
    "\n",
    "filtered_dataset = cleaned_dataset[columns]\n",
    "\n",
    "#Saves the filtered dataset\n",
    "filtered_dataset.to_csv(\"/Users/maks/Documents/ML_project/Filtered_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below contain code that was used to preprocess the data and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads in the cleaned and reduced MET dataset\n",
    "dataset = pd.read_csv(\"/Users/maks/Documents/ML_project/Filtered_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets up labels from the loaded in .csv file to prepare features next\n",
    "feature_columns = [f\"hurs_{i}\" for i in range(1, 13)] + [f\"psl_{i}\" for i in range(1, 13)] + [f\"tas_{i}\" for i in range(1, 13)] + [f\"rainfall_{i}\" for i in range(1, 13)]\n",
    "\n",
    "#Prepares the featurees to get them into data sequences\n",
    "data = dataset[feature_columns]\n",
    "\n",
    "#Prepares data sequences\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(data) - 1):\n",
    "    x.append(data.iloc[i].values)\n",
    "    y.append(data.iloc[i + 1][\"hurs_1\"])    #Used to predict next month's relative humidity\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits data into training and testing sets. test_size controls the size of data put towards testing, here 0.2\n",
    "#means that 20% will be used for testing. Specifying random_state allows for reproducible output across cells or\n",
    "#runs.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 13)\n",
    "\n",
    "#Initializes scaler\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "#Fits scaler to training data\n",
    "scaled_x_train = x_scaler.fit_transform(x_train)\n",
    "scaled_y_train = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "#Fits scaler to testing data\n",
    "scaled_x_test = x_scaler.transform(x_test)\n",
    "scaled_y_test = y_scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "#Checks the sizes of x and y training and testing arrays\n",
    "print(\"Shape of scaled_x_train:\", scaled_x_train.shape)\n",
    "print(\"Shape of scaled_y_train:\", scaled_y_train.shape)\n",
    "print(\"Shape of scaled_x_test:\", scaled_x_test.shape)\n",
    "print(\"Shape of scaled_y_test:\", scaled_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifies the shape of the input matrix\n",
    "input_shape = scaled_x_train.shape[1]\n",
    "\n",
    "#Builds the MLP model. Dropout and Lasso regularization are used to prevent overfitting. Dropout rate is \n",
    "# varied depending on how deep the layer is. \n",
    "model = Sequential([\n",
    "    Input(shape = (input_shape,)),\n",
    "    Dense(256, activation = \"relu\", kernel_regularizer = l2(0.006)), \n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation = \"relu\", kernel_regularizer = l2(0.006)), \n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation = \"relu\", kernel_regularizer = l2(0.006)), \n",
    "    Dropout(0.4),\n",
    "    Dense(32, activation = \"relu\", kernel_regularizer = l2(0.006)), \n",
    "    Dropout(0.5),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiles the model\n",
    "model.compile(optimizer = \"adam\", loss = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares a stopper to allow for a larger number of epochs but to help avoid overfitting\n",
    "stop = EarlyStopping(monitor = \"val_loss\", patience = 10, restore_best_weights = True)\n",
    "\n",
    "class_weight = {0: 1., 1: 10.}\n",
    "\n",
    "#Trains the model\n",
    "model.fit(scaled_x_train, scaled_y_train, epochs = 100, batch_size = 32, class_weight = class_weight, validation_data = (scaled_x_test, scaled_y_test), callbacks = [stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluates the model in a simple way\n",
    "loss = model.evaluate(scaled_x_test, scaled_y_test)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_scaled = model.predict(scaled_x_test)\n",
    "predictions = y_scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {predictions[i] [0]}, Actual: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below includes code that saved predicted values and corresponding actual values fo relative humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves test values and corresponding predictions.\n",
    "results_df = pd.DataFrame({\n",
    "    \"true_label\": y_test.flatten(),\n",
    "    \"prediction\": predictions.flatten()\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"/Users/maks/Documents/ML_project/model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below includes code that was used to calculate evaluation metrcis of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code below evaluates the model using multiple metrics.\n",
    "\n",
    "#Distance metrics\n",
    "mean_absolute = mean_absolute_error(y_test, predictions)\n",
    "mean_squared = mean_squared_error(y_test, predictions)\n",
    "root_mean = mean_squared_error(y_test, predictions, squared = False)\n",
    "\n",
    "#Correlation metrics\n",
    "pearson = pearsonr(y_test, predictions.flatten())\n",
    "spearman = spearmanr(y_test, predictions)\n",
    "\n",
    "#Displays the results\n",
    "print(f\"Mean absolute error of the model: {mean_absolute}\")\n",
    "print(f\"Mean squared error of the model: {mean_squared}\")\n",
    "print(f\"Root mean sqared error of the model: {root_mean}\")\n",
    "print(f\"Pearson correlation coefficient of the model: {pearson}\")\n",
    "print(f\"Spearman rank correlation of the model: {spearman}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
